{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AdaHessian(optim.Optimizer):\n",
    "    def __init__(self,params,wd = 0, mc_iters=1, lr=.001, betas=(.9,.999),eps = 1e-8, control_variate=False):\n",
    "        super(AdaHessian, self).__init__(params, defaults={'lr':lr})\n",
    "        self.state = dict()\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.control_variate = control_variate\n",
    "        self.eps = eps\n",
    "        self.mc_iters = mc_iters\n",
    "        self.n_steps = 0\n",
    "        self.wd = wd\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                self.state[p] = dict(mom=th.zeros_like(p.data),hess_mom=th.zeros_like(p.data))\n",
    "                p.hess = th.zeros_like(p.data)\n",
    "    def zero_hessian(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                p.hess = th.zeros_like(p.data)\n",
    "\n",
    "    def set_hessian(self):\n",
    "        vals = []\n",
    "        params = []\n",
    "        if self.control_variate:\n",
    "            for group in self.param_groups:\n",
    "                for p in group['params']:\n",
    "                    params.append(p)\n",
    "                    vals.append(p.grad - self.state[p][\"hess_mom\"].detach() * p * self.betas[1])\n",
    "        else:\n",
    "            for group in self.param_groups:\n",
    "                for p in group['params']:\n",
    "                    params.append(p)\n",
    "                    vals.append(p.grad)\n",
    "\n",
    "\n",
    "        for iter in range(self.mc_iters):\n",
    "          with th.no_grad():\n",
    "            z_values = [th.randn_like(p.data) for p in params]\n",
    "            hz_values = th.autograd.grad(vals, params,z_values,retain_graph = (iter != self.mc_iters - 1))\n",
    "            for p, z, hz in zip(params, z_values, hz_values):\n",
    "                p.hess += hz * z / self.mc_iters\n",
    "\n",
    "    def step(self,closure = None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "          loss = closure()\n",
    "        self.n_steps += 1\n",
    "        self.zero_hessian()\n",
    "        self.set_hessian()\n",
    "        beta0 = self.betas[0]\n",
    "        beta1 = self.betas[1]\n",
    "        bias_correction_0 = 1-beta0**self.n_steps\n",
    "        bias_correction_1 = 1-beta1**self.n_steps\n",
    "        with th.no_grad():\n",
    "          for group in self.param_groups:\n",
    "              step_size = group['lr']*bias_correction_0\n",
    "              for p in group['params']:\n",
    "                  if self.wd != 0:\n",
    "                      p.mul_(1-self.wd*self.lr)\n",
    "\n",
    "                  mom = self.state[p]['mom']\n",
    "                  mom.mul_(beta0).add_(p.grad,alpha=1-beta0)\n",
    "                  hess_mom = self.state[p]['hess_mom']\n",
    "                  hess_mom.mul_(beta1).add_(p.hess,alpha=1-beta1)\n",
    "                  denominator = th.abs(hess_mom/bias_correction_1).pow(1/2).add_(self.eps)\n",
    "                  p.addcdiv_(mom,denominator, value=-step_size)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_accuracy(y_hat, y):\n",
    "    return th.sum(th.where(th.argmax(y_hat,-1) == y,1, 0 ))/y_hat.size(0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, train_dataloader, optimizer, epoch):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    average_loss = 0\n",
    "    average_accuracy = 0\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x,y = batch\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat,y)\n",
    "        loss.backward(create_graph=True)\n",
    "        accuracy = compute_accuracy(y_hat,y)\n",
    "        optimizer.step()\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"epoch: {epoch} idx: {idx} \\t loss = {loss} , accuracy = {accuracy*100}%\")\n",
    "        average_loss += loss\n",
    "        average_accuracy += accuracy\n",
    "    return average_loss / len(train_dataloader) , average_accuracy / len(train_dataloader)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, val_dataloader ):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    average_loss = 0\n",
    "    average_accuracy = 0\n",
    "    for idx, batch in enumerate(val_dataloader):\n",
    "        x,y = batch\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat,y)\n",
    "        accuracy = compute_accuracy(y_hat, y)\n",
    "        average_loss += loss\n",
    "        average_accuracy += accuracy\n",
    "    return average_loss / len(val_dataloader) , average_accuracy / len(val_dataloader)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def experiment():\n",
    "    epochs = 20\n",
    "    bsz = 64\n",
    "    lr = .001\n",
    "    model = tv.models.resnet18()\n",
    "    model.cuda()\n",
    "    optimizer = AdaHessian(model.parameters(),lr=lr,wd=.01)\n",
    "\n",
    "    train_transform = tv.transforms.Compose([\n",
    "    tv.transforms.RandomHorizontalFlip(),\n",
    "    tv.transforms.RandomCrop(32, padding=4),\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "    val_transform = tv.transforms.Compose([\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "    trainset = tv.datasets.CIFAR10(\"\\data\", download=True,transform=train_transform)\n",
    "    valset = tv.datasets.CIFAR10(\"\\data\", train=False, download=True,transform=val_transform)\n",
    "    train_loader = th.utils.data.DataLoader(trainset, batch_size=bsz, shuffle=True)\n",
    "    val_loader = th.utils.data.DataLoader(valset, batch_size=bsz, shuffle=False)\n",
    "\n",
    "\n",
    "    for i in range(epochs):\n",
    "        train_loss, train_accuracy = train_one_epoch(model,train_loader,optimizer,i)\n",
    "        val_loss, val_accuracy = evaluate(model,val_loader)\n",
    "        print(f\"Epoch [{i}/{epochs}]: train_loss={train_loss}, val_loss={val_loss}, train_acc={train_accuracy*100}%, val_acc={val_accuracy*100}%\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
